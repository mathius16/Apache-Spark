{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INSTAL SPARK, JAVA DAN HADOOP"
      ],
      "metadata": {
        "id": "pSaMN863rur6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "9Tu9NkgWmtwP",
        "outputId": "cfb62535-da9c-479c-86e5-0666453dd7be"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 2s (137 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data\t\t\t spark-3.1.1-bin-hadoop3.2.tgz.4  u2.base  u5.base  u.data\n",
            "spark-3.1.1-bin-hadoop3.2\t spark-3.1.1-bin-hadoop3.2.tgz.5  u2.test  u5.test  u.genre\n",
            "spark-3.1.1-bin-hadoop3.2.tgz\t spark-3.1.1-bin-hadoop3.2.tgz.6  u3.base  ua.base  u.info\n",
            "spark-3.1.1-bin-hadoop3.2.tgz.1  spark-3.1.1-bin-hadoop3.2.tgz.7  u3.test  ua.test  u.item\n",
            "spark-3.1.1-bin-hadoop3.2.tgz.2  u1.base\t\t\t  u4.base  ub.base  u.occupation\n",
            "spark-3.1.1-bin-hadoop3.2.tgz.3  u1.test\t\t\t  u4.test  ub.test  u.user\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d8af4285630>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f50a00011d14:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8mSIrXvhr5qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat kesimpulan rating film dari 1-5"
      ],
      "metadata": {
        "id": "TF1z9sdwr7rb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEj6cD4LmhhA",
        "outputId": "b7d75c77-0bc8-4f66-eec0-68f3b0c32737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 6110\n",
            "2 11370\n",
            "3 27145\n",
            "4 34174\n",
            "5 21201\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "try:\n",
        "    sc = SparkContext.getOrCreate()\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "    raise SystemExit(1)\n",
        "\n",
        "lines = sc.textFile(\"u.data\")\n",
        "ratings = lines.map(lambda x: x.split()[2])\n",
        "result = ratings.countByValue()\n",
        "\n",
        "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
        "for key, value in sortedResults.items():\n",
        "    print(\"%s %i\" % (key, value))\n"
      ]
    }
  ]
}